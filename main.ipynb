{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import scipy.io as sio\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.7\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "def preprocess_data(eeg_data):\n",
    "\n",
    "    #standardization\n",
    "    scaler = StandardScaler()\n",
    "    preprocessed_data = scaler.fit_transform(eeg_data)\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(preprocessed_data)\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Amplitude')\n",
    "    # plt.title('Preprocessed EEG Data')\n",
    "    # plt.show()\n",
    "\n",
    "    # Reshape the data into a 2D array for feature scaling\n",
    "    reshaped_data = np.reshape(preprocessed_data, (12*7*5120, 5))\n",
    "\n",
    "    # Perform feature scaling using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    minmax_scaled_data = minmax_scaler.fit_transform(reshaped_data)\n",
    "\n",
    "    # Perform feature scaling using RobustScaler\n",
    "    robust_scaler = RobustScaler()\n",
    "    robust_scaled_data = robust_scaler.fit_transform(reshaped_data)\n",
    "\n",
    "    # Reshape the scaled data back to its original shape\n",
    "    minmax_scaled_data = np.reshape(minmax_scaled_data, (12, 7, 5120, 5))\n",
    "    robust_scaled_data = np.reshape(robust_scaled_data, (12, 7, 5120, 5))\n",
    "\n",
    "    # Verify the shape of the scaled data\n",
    "    print(\"MinMax scaled data shape:\", minmax_scaled_data.shape)\n",
    "    print(\"Robust scaled data shape:\", robust_scaled_data.shape)\n",
    "\n",
    "    return minmax_scaled_data, robust_scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(eeg_data):\n",
    "    mean_features = np.mean(eeg_data, axis=1)\n",
    "    std_features = np.std(eeg_data, axis=1)\n",
    "    variance_features = np.var(eeg_data, axis=1)\n",
    "    skewness_features = scipy.stats.skew(eeg_data, axis=1)\n",
    "    kurtosis_features = scipy.stats.kurtosis(eeg_data, axis=1)\n",
    "\n",
    "    # tsallis_entropy_features = scipy.stats.entropy(eeg_data, axis=1)\n",
    "    # renyi_entropy_features = scipy.stats.entropy(eeg_data, axis=1, base=2)\n",
    "    # shannon_entropy_features = scipy.stats.entropy(eeg_data, axis=1, base=np.exp(1))\n",
    "    # log_energy_features = np.log(np.sum(np.square(eeg_data), axis=1))\n",
    "    \n",
    "    extracted_features = np.column_stack((mean_features, std_features, variance_features, skewness_features, kurtosis_features))\n",
    "    return extracted_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mat_file(file_name):\n",
    "    mat = sio.loadmat(file_name)\n",
    "    mat = mat[\"ssvep\"]\n",
    "    data = []\n",
    "    for stimuli in mat:\n",
    "        stimuli_name = stimuli[1][0]\n",
    "        stimuli_data = np.array(stimuli[0])\n",
    "        stimuli_data = np.delete(stimuli_data, -1, axis=1)\n",
    "        stimuli_data = preprocess_data(stimuli_data)\n",
    "        stimuli_features = extract_features(stimuli_data)\n",
    "        data.append(stimuli_features)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels, test_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, stratify=labels)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 112640 into shape (430080,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m labels:\n\u001b[0;32m      8\u001b[0m     file_name \u001b[39m=\u001b[39m volunteer \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m label \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.mat\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     stimuli_features \u001b[39m=\u001b[39m read_mat_file(file_name)\n\u001b[0;32m     10\u001b[0m     data\u001b[39m.\u001b[39mappend(stimuli_features)\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m, in \u001b[0;36mread_mat_file\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      7\u001b[0m stimuli_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(stimuli[\u001b[39m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m stimuli_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(stimuli_data, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m stimuli_data \u001b[39m=\u001b[39m preprocess_data(stimuli_data)\n\u001b[0;32m     10\u001b[0m stimuli_features \u001b[39m=\u001b[39m extract_features(stimuli_data)\n\u001b[0;32m     11\u001b[0m data\u001b[39m.\u001b[39mappend(stimuli_features)\n",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(eeg_data)\u001b[0m\n\u001b[0;32m      6\u001b[0m preprocessed_data \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(eeg_data)\n\u001b[0;32m      7\u001b[0m \u001b[39m# plt.figure(figsize=(10, 6))\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# plt.plot(preprocessed_data)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# plt.xlabel('Time')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[39m# Reshape the data into a 2D array for feature scaling\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m reshaped_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreshape(preprocessed_data, (\u001b[39m12\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m7\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m5120\u001b[39;49m, \u001b[39m5\u001b[39;49m))\n\u001b[0;32m     17\u001b[0m \u001b[39m# Perform feature scaling using MinMaxScaler\u001b[39;00m\n\u001b[0;32m     18\u001b[0m minmax_scaler \u001b[39m=\u001b[39m MinMaxScaler()\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 112640 into shape (430080,5)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    volunteers = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "    labels = [\"A\", \"B\"]\n",
    "    data = []\n",
    "    data_labels = []\n",
    "    for volunteer in volunteers:\n",
    "        for label in labels:\n",
    "            file_name = volunteer + \"_\" + label + \".mat\"\n",
    "            stimuli_features = read_mat_file(file_name)\n",
    "            data.append(stimuli_features)\n",
    "            if label == \"A\":\n",
    "                data_labels.append(1)\n",
    "            else:\n",
    "                data_labels.append(0)\n",
    "    print(\"Preprocessed data format:\")\n",
    "    print(np.array(data).shape)\n",
    "    print(data_labels)\n",
    "    print(\"\")\n",
    "    X = np.array(data)\n",
    "    y = np.array(data_labels)\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X.shape[2]\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold: [ 3  4  5  6  7  8  9 10 11] Validating on fold: [0 1 2]\n",
      "Training on fold: [ 0  1  2  6  7  8  9 10 11] Validating on fold: [3 4 5]\n",
      "Training on fold: [ 0  1  2  3  4  5  8  9 10 11] Validating on fold: [6 7]\n",
      "Training on fold: [ 0  1  2  3  4  5  6  7 10 11] Validating on fold: [8 9]\n",
      "Training on fold: [0 1 2 3 4 5 6 7 8 9] Validating on fold: [10 11]\n",
      "Average Accuracy: 86.66666666666666\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in kf.split(X):\n",
    "    print(\"Training on fold:\", train_index, \"Validating on fold:\", val_index)\n",
    "    X_train_fold, X_val_fold = torch.from_numpy(X[train_index]), torch.from_numpy(X[val_index])\n",
    "    y_train_fold, y_val_fold = torch.from_numpy(y[train_index]), torch.from_numpy(y[val_index])\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train_fold.float())\n",
    "    loss = criterion(outputs.squeeze(), y_train_fold.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_fold.float())\n",
    "        val_predictions = (val_outputs.squeeze() > 0.5).float()\n",
    "        accuracy = accuracy_score(y_val_fold, val_predictions.numpy())\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(\"Average Accuracy:\", average_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGClassifierCNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(EEGClassifierCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * (input_shape[0] // 2 - 1) * (input_shape[1] // 2 - 1), 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, train_data, train_labels, batch_size, epochs):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "        train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "        train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self(inputs.unsqueeze(1))\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1} Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    def evaluate(self, test_data, test_labels):\n",
    "        test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "        test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        test_data = test_data.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self(test_data.unsqueeze(1))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == test_labels).sum().item()\n",
    "            accuracy = correct / test_labels.size(0)\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    def predict(self, data):\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        data = data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self(data.unsqueeze(1))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        return predicted.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[0;32m     10\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m---> 11\u001b[0m model\u001b[39m.\u001b[39mtrain(train_data, train_labels, batch_size, epochs)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[39m.\u001b[39mevaluate(test_data, test_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have your EEG data and labels stored in train_data, train_labels, test_data, and test_labels\n",
    "\n",
    "# Create an instance of the EEGClassifierCNN\n",
    "input_shape = (7, 5120)  # Specify the input shape of your EEG data\n",
    "num_classes = 2  # Specify the number of classes (e.g., normal vs. caffeinated)\n",
    "model = EEGClassifierCNN(input_shape, num_classes)\n",
    "\n",
    "# Train the model\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "model.train(train_data, train_labels, batch_size, epochs)\n",
    "\n",
    "# Evaluate the model \n",
    "model.evaluate(test_data, test_labels)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Anne/Facultate/licenta/ScienceDirect_files_16Dec2022_10-38-19/CSV\\sig10Hz_V1_A.csv') as f:\n",
    "#     train = pd.read_csv(f)\n",
    "# train = train.drop('ECG', axis=1)\n",
    "# train.head(5120)\n",
    "\n",
    "# with open('/Anne/Facultate/licenta/ScienceDirect_files_16Dec2022_10-38-19/CSV\\sig10Hz_V1_B.csv') as f:\n",
    "#     test = pd.read_csv(f)\n",
    "# # test = test.drop('ECG', axis=1)\n",
    "# test.head(5120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_train = list(train.columns)\n",
    "# train.columns = new_train\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=22, ncols=1, figsize=(20, 40), sharey = True)\n",
    "\n",
    "# for i, col in enumerate(train.columns):\n",
    "#   ax = axes[i]\n",
    "#   ax.plot(train[col])\n",
    "#   ax.set_title(col)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# # plt.show\n",
    "# plt.savefig(\"pictures/EEG1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/Anne/Facultate/licenta/Dataset/CSV'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            file_name = os.path.join(directory, filename)\n",
    "            # print (file_name)\n",
    "#5120 DATA POINTS - 20s duration of 7 photic stimulus and resting period of 10s in between stimuli freq\n",
    "column_names = [\"FP1\", \"FP2\", \"PG1\", \"PG2\", \"F7\", \"F3\", \"F2\", \"F4\", \"F8\", \"C3\", \"C2\", \"C4\", \"P3\", \"P4\", \"P2\", \"T3\", \"T4\", \"T5\", \"T6\", \"O1\", \"OZ\", \"O2\", \"ECG\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(4608, 128)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
